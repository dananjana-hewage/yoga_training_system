{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
    }
   },
   "source": [
    "import math\n",
    "from pickle import FALSE\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.ma.core import left_shift"
   ],
   "outputs": [],
  },
  {
   "metadata": {
    "ExecuteTime": {
    }
   },
   "cell_type": "code",
   "source": [
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "pose = mp_pose.Pose(\n",
    "    static_image_mode=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    model_complexity = 1\n",
    ")"
   ],
   "id": "31ff8a89fed4975e",
   "outputs": [],
  },
  {
   "metadata": {
    }
   },
   "cell_type": "code",
   "source": [
    "def detectPose(image, pose, display=True):\n",
    "    '''\n",
    "    Perform pose detection on an image.\n",
    "    Args:\n",
    "        image: Input image with a prominent person for pose detection.\n",
    "        pose: Mediapipe pose function for detection.\n",
    "        display: If True, display the images and landmarks.\n",
    "    Returns:\n",
    "        output_image: Image with detected pose landmarks drawn.\n",
    "        landmarks: List of detected landmarks in original scale.\n",
    "    '''\n",
    "    # Create a copy of the input image.\n",
    "    output_image = image.copy()\n",
    "\n",
    "    # Convert BGR to RGB format.\n",
    "    imageRGB = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Perform Pose Detection.\n",
    "    results = pose.process(imageRGB)\n",
    "\n",
    "    # Retrieve the image dimensions.\n",
    "    height, width, _ = image.shape\n",
    "\n",
    "    # Initialize list for landmarks.\n",
    "    landmarks = []\n",
    "\n",
    "    # If landmarks are detected.\n",
    "    if results.pose_landmarks:\n",
    "        # Draw Pose landmarks on the image.\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image=output_image,\n",
    "            landmark_list=results.pose_landmarks,\n",
    "            connections=mp_pose.POSE_CONNECTIONS\n",
    "        )\n",
    "\n",
    "        # Save the landmarks.\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            landmarks.append((int(landmark.x * width), int(landmark.y * height), landmark.z * width))\n",
    "\n",
    "    # Display results if requested.\n",
    "    if display:\n",
    "        plt.figure(figsize=[22, 22])\n",
    "        plt.subplot(121);\n",
    "        plt.imshow(image[:, :, ::-1]);\n",
    "        plt.title(\"Original Image\");\n",
    "        plt.axis('off');\n",
    "        plt.subplot(122);\n",
    "        plt.imshow(output_image[:, :, ::-1]);\n",
    "        plt.title(\"Output Image\");\n",
    "        plt.axis('off');\n",
    "        mp_drawing.plot_landmarks(results.pose_world_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    else:\n",
    "        return output_image, landmarks"
   ],
   "id": "50295e4f73bca545",
   "outputs": [],
  },
  {
   "metadata": {
    "ExecuteTime": {
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Initialize Pose function for video.\n",
    "pose_video = mp_pose.Pose(\n",
    "    static_image_mode=False,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5,\n",
    "    model_complexity=1\n",
    ")\n",
    "\n",
    "# Open the webcam (change index to 1 if another camera is used).\n",
    "video = cv2.VideoCapture(0)\n",
    "\n",
    "# Create named window for resizing.\n",
    "cv2.namedWindow('Pose Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Set video resolution.\n",
    "video.set(3, 1280)  # Width\n",
    "video.set(4, 960)  # Height\n",
    "\n",
    "# Initialize time for FPS calculation.\n",
    "time1 = 0\n",
    "\n",
    "# Video capture loop.\n",
    "while video.isOpened():\n",
    "    # Read a frame.\n",
    "    ok, frame = video.read()\n",
    "\n",
    "    # If frame is not read properly, break the loop.\n",
    "    if not ok:\n",
    "        print(\"Error: Unable to read frame.\")\n",
    "        break\n",
    "\n",
    "    # Flip the frame horizontally for natural view.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "\n",
    "    # Resize the frame.\n",
    "    frame_height, frame_width, _ = frame.shape\n",
    "    frame = cv2.resize(frame, (int(frame_width * (640 / frame_height)), 640))\n",
    "\n",
    "    # Perform pose detection.\n",
    "    try:\n",
    "        # Convert the frame to RGB for processing.\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Perform pose detection.\n",
    "        results = pose_video.process(rgb_frame)\n",
    "\n",
    "        # If landmarks are detected, process them.\n",
    "        if results.pose_landmarks:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Print and optionally process landmarks.\n",
    "            for i, landmark in enumerate(landmarks):\n",
    "                print(\n",
    "                    f\"Landmark {i}: x={landmark.x:.3f}, y={landmark.y:.3f}, z={landmark.z:.3f}, visibility={landmark.visibility:.2f}\"\n",
    "                )\n",
    "\n",
    "                # Optionally, convert normalized coordinates to pixels.\n",
    "                pixel_x = int(landmark.x * frame_width)\n",
    "                pixel_y = int(landmark.y * frame_height)\n",
    "                print(f\"Landmark {i} in pixels: x={pixel_x}, y={pixel_y}\")\n",
    "\n",
    "            # Draw pose landmarks on the frame.\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                results.pose_landmarks,\n",
    "                mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during pose detection:\", e)\n",
    "        continue\n",
    "\n",
    "    # Calculate and display FPS.\n",
    "    time2 = time()\n",
    "    if (time2 - time1) > 0:\n",
    "        fps = 1.0 / (time2 - time1)\n",
    "        cv2.putText(frame, f'FPS: {int(fps)}', (10, 30), cv2.FONT_HERSHEY_PLAIN, 2, (0, 255, 0), 3)\n",
    "    time1 = time2\n",
    "\n",
    "    # Display the frame.\n",
    "    cv2.imshow('Pose Detection', frame)\n",
    "\n",
    "    # Break on 'ESC' key press.\n",
    "    if cv2.waitKey(1) & 0xFF == 27:\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close windows.\n",
    "video.release()\n",
    "cv2.destroyAllWindows()\n"
   ],
   "id": "ea4d6b015523bf31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Landmark 13 in pixels: x=866, y=933\n",
      "Landmark 28 in pixels: x=395, y=2394\n",
      "Landmark 28 in pixels: x=406, y=2341\n",
      "Landmark 3 in pixels: x=657, y=264\n",
      "Landmark 0 in pixels: x=582, y=338\n",
      "Landmark 11 in pixels: x=737, y=657\n",
      "Landmark 17 in pixels: x=862, y=1188\n",
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001B[0m, in \u001B[0;36mPose.process\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NamedTuple:\n\u001B[0;32m    165\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001B[39;00m\n\u001B[0;32m    166\u001B[0m \n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;124;03m         \"enable_segmentation\" is set to true.\u001B[39;00m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    186\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpose_landmarks:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m landmark \u001B[38;5;129;01min\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpose_landmarks\u001B[38;5;241m.\u001B[39mlandmark:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001B[0m, in \u001B[0;36mSolutionBase.process\u001B[1;34m(self, input_data)\u001B[0m\n\u001B[0;32m    334\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39madd_packet_to_input_stream(\n\u001B[0;32m    336\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream_name,\n\u001B[0;32m    337\u001B[0m         packet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_packet(input_stream_type,\n\u001B[0;32m    338\u001B[0m                                  data)\u001B[38;5;241m.\u001B[39mat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_simulated_timestamp))\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_until_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001B[39;00m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;66;03m# output stream names.\u001B[39;00m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_stream_type_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
  },
  {
   "metadata": {
    "ExecuteTime": {
    }
   },
   "cell_type": "code",
   "source": [
    "for lndmrk in mp_pose.PoseLandmark:\n",
    "    print(lndmrk)"
   ],
   "id": "6b8a937409192f50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoseLandmark.NOSE\n",
      "PoseLandmark.LEFT_EYE_INNER\n",
      "PoseLandmark.LEFT_EYE\n",
      "PoseLandmark.LEFT_EYE_OUTER\n",
      "PoseLandmark.RIGHT_EYE_INNER\n",
      "PoseLandmark.RIGHT_EYE\n",
      "PoseLandmark.RIGHT_EYE_OUTER\n",
      "PoseLandmark.LEFT_EAR\n",
      "PoseLandmark.RIGHT_EAR\n",
      "PoseLandmark.MOUTH_LEFT\n",
      "PoseLandmark.MOUTH_RIGHT\n",
      "PoseLandmark.LEFT_SHOULDER\n",
      "PoseLandmark.RIGHT_SHOULDER\n",
      "PoseLandmark.LEFT_ELBOW\n",
      "PoseLandmark.RIGHT_ELBOW\n",
      "PoseLandmark.LEFT_WRIST\n",
      "PoseLandmark.RIGHT_WRIST\n",
      "PoseLandmark.LEFT_PINKY\n",
      "PoseLandmark.RIGHT_PINKY\n",
      "PoseLandmark.LEFT_INDEX\n",
      "PoseLandmark.RIGHT_INDEX\n",
      "PoseLandmark.LEFT_THUMB\n",
      "PoseLandmark.RIGHT_THUMB\n",
      "PoseLandmark.LEFT_HIP\n",
      "PoseLandmark.RIGHT_HIP\n",
      "PoseLandmark.LEFT_KNEE\n",
      "PoseLandmark.RIGHT_KNEE\n",
      "PoseLandmark.LEFT_ANKLE\n",
      "PoseLandmark.RIGHT_ANKLE\n",
      "PoseLandmark.LEFT_HEEL\n",
      "PoseLandmark.RIGHT_HEEL\n",
      "PoseLandmark.LEFT_FOOT_INDEX\n",
      "PoseLandmark.RIGHT_FOOT_INDEX\n"
     ]
    }
   ],
  },
  {
   "metadata": {
    "ExecuteTime": {
    }
   },
   "cell_type": "code",
   "source": "landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n",
   "id": "7d415f5f61a22190",
   "outputs": [
    {
     "data": {
      "text/plain": [
      ]
     },
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
  },
  {
   "metadata": {
    "ExecuteTime": {
    }
   },
   "cell_type": "code",
   "source": [
    "#lets see this one\n",
    "def calculate_angle(a, b, c):\n",
    "    a = np.array(a)  # First point\n",
    "    b = np.array(b)  # Midpoint\n",
    "    c = np.array(c)  # End point\n",
    "    \n",
    "    radians = np.arctan2(c[1] - b[1], c[0] - b[0]) - np.arctan2(a[1] - b[1], a[0] - b[0])\n",
    "    angle = np.abs(radians * 180.0 / np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360 - angle\n",
    "        \n",
    "    return angle \n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Extract relevant points\n",
    "            joint_sets = {\n",
    "                \"Left Arm\": [mp_pose.PoseLandmark.LEFT_SHOULDER, \n",
    "                             mp_pose.PoseLandmark.LEFT_ELBOW, \n",
    "                             mp_pose.PoseLandmark.LEFT_WRIST],\n",
    "                \n",
    "                \"Right Arm\": [mp_pose.PoseLandmark.RIGHT_SHOULDER, \n",
    "                              mp_pose.PoseLandmark.RIGHT_ELBOW, \n",
    "                              mp_pose.PoseLandmark.RIGHT_WRIST],\n",
    "\n",
    "                \"Left Leg\": [mp_pose.PoseLandmark.LEFT_HIP, \n",
    "                             mp_pose.PoseLandmark.LEFT_KNEE, \n",
    "                             mp_pose.PoseLandmark.LEFT_ANKLE],\n",
    "\n",
    "                \"Right Leg\": [mp_pose.PoseLandmark.RIGHT_HIP, \n",
    "                              mp_pose.PoseLandmark.RIGHT_KNEE, \n",
    "                              mp_pose.PoseLandmark.RIGHT_ANKLE],\n",
    "\n",
    "                \"Torso\": [mp_pose.PoseLandmark.LEFT_SHOULDER, \n",
    "                          mp_pose.PoseLandmark.LEFT_HIP, \n",
    "                          mp_pose.PoseLandmark.LEFT_KNEE]\n",
    "            }\n",
    "\n",
    "            for joint_name, points in joint_sets.items():\n",
    "                p1 = [landmarks[points[0].value].x, landmarks[points[0].value].y]\n",
    "                p2 = [landmarks[points[1].value].x, landmarks[points[1].value].y]\n",
    "                p3 = [landmarks[points[2].value].x, landmarks[points[2].value].y]\n",
    "                \n",
    "                angle = calculate_angle(p1, p2, p3)\n",
    "                \n",
    "                # Print angle values to console\n",
    "                print(f\"{joint_name} angle: {angle:.2f}\")\n",
    "\n",
    "                # Visualize angle on screen\n",
    "                cv2.putText(image, f\"{joint_name}: {int(angle)}\", \n",
    "                            tuple(np.multiply(p2, [640, 480]).astype(int)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Render pose landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2))\n",
    "\n",
    "        cv2.imshow('Mediapipe Pose Estimation', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "id": "de5571edb09b2ce1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torso angle: 175.99\n",
      "Right Leg angle: 178.99\n",
      "Torso angle: 175.55\n",
      "Right Leg angle: 178.95\n",
      "Torso angle: 175.17\n",
      "Left Leg angle: 178.70\n",
      "Torso angle: 175.71\n",
      "Right Leg angle: 178.90\n",
      "Left Leg angle: 178.71\n",
      "Right Leg angle: 178.78\n",
      "Torso angle: 176.34\n",
      "Right Leg angle: 178.61\n",
      "Right Arm angle: 167.06\n",
      "Left Leg angle: 178.57\n",
      "Right Leg angle: 179.20\n",
      "Left Leg angle: 178.60\n",
      "Torso angle: 177.04\n",
      "Left Leg angle: 178.68\n",
      "Right Leg angle: 179.18\n",
      "Right Leg angle: 179.16\n",
      "Left Leg angle: 178.72\n",
      "Left Leg angle: 178.65\n",
      "Right Leg angle: 179.26\n",
      "Torso angle: 177.22\n",
      "Left Leg angle: 178.42\n",
      "Right Leg angle: 179.98\n",
      "Left Leg angle: 177.76\n",
      "Left Leg angle: 177.46\n",
      "Right Arm angle: 168.04\n",
      "Left Leg angle: 177.88\n",
      "Right Leg angle: 179.62\n",
      "Right Leg angle: 179.57\n",
      "Torso angle: 178.37\n",
      "Left Leg angle: 177.42\n",
      "Right Leg angle: 179.45\n",
      "Torso angle: 178.48\n",
      "Right Arm angle: 169.38\n",
      "Right Leg angle: 178.97\n",
      "Right Arm angle: 170.43\n",
      "Left Leg angle: 175.81\n",
      "Right Arm angle: 170.45\n",
      "Right Arm angle: 170.97\n",
      "Right Arm angle: 171.04\n",
      "Right Leg angle: 175.02\n",
      "Torso angle: 179.79\n",
      "Right Arm angle: 171.07\n",
      "Right Arm angle: 171.58\n",
      "Torso angle: 178.78\n",
      "Left Leg angle: 170.27\n",
      "Left Leg angle: 170.91\n",
      "Right Arm angle: 169.23\n",
      "Torso angle: 178.36\n",
      "Left Leg angle: 168.63\n",
      "Left Leg angle: 168.73\n",
      "Torso angle: 177.28\n",
      "Right Leg angle: 167.36\n",
      "Torso angle: 179.40\n",
      "Torso angle: 178.60\n",
      "Torso angle: 178.37\n",
      "Torso angle: 178.49\n",
      "Right Arm angle: 172.48\n",
      "Left Leg angle: 174.26\n",
      "Torso angle: 178.54\n",
      "Right Leg angle: 173.64\n",
      "Right Leg angle: 172.49\n",
      "Torso angle: 178.47\n",
      "Right Leg angle: 172.43\n",
      "Torso angle: 178.48\n",
      "Right Leg angle: 174.83\n",
      "Left Leg angle: 174.44\n",
      "Left Leg angle: 174.05\n",
      "Torso angle: 179.48\n",
      "Right Leg angle: 174.27\n",
      "Torso angle: 179.27\n",
      "Torso angle: 179.59\n",
      "Torso angle: 177.27\n",
      "Left Leg angle: 173.51\n",
      "Torso angle: 178.77\n",
      "Torso angle: 177.17\n",
      "Torso angle: 178.40\n",
      "Left Leg angle: 173.96\n",
      "Torso angle: 179.52\n",
      "Torso angle: 177.35\n",
      "Right Leg angle: 172.73\n",
      "Torso angle: 179.11\n",
      "Torso angle: 178.70\n",
      "Torso angle: 178.48\n",
      "Right Arm angle: 167.47\n",
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solutions\\pose.py:185\u001B[0m, in \u001B[0;36mPose.process\u001B[1;34m(self, image)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess\u001B[39m(\u001B[38;5;28mself\u001B[39m, image: np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m NamedTuple:\n\u001B[0;32m    165\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\u001B[39;00m\n\u001B[0;32m    166\u001B[0m \n\u001B[0;32m    167\u001B[0m \u001B[38;5;124;03m  Args:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;124;03m         \"enable_segmentation\" is set to true.\u001B[39;00m\n\u001B[0;32m    183\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m   results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocess\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mimage\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    186\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpose_landmarks:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m landmark \u001B[38;5;129;01min\u001B[39;00m results\u001B[38;5;241m.\u001B[39mpose_landmarks\u001B[38;5;241m.\u001B[39mlandmark:  \u001B[38;5;66;03m# pytype: disable=attribute-error\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\mediapipe\\python\\solution_base.py:340\u001B[0m, in \u001B[0;36mSolutionBase.process\u001B[1;34m(self, input_data)\u001B[0m\n\u001B[0;32m    334\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    335\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39madd_packet_to_input_stream(\n\u001B[0;32m    336\u001B[0m         stream\u001B[38;5;241m=\u001B[39mstream_name,\n\u001B[0;32m    337\u001B[0m         packet\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_packet(input_stream_type,\n\u001B[0;32m    338\u001B[0m                                  data)\u001B[38;5;241m.\u001B[39mat(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_simulated_timestamp))\n\u001B[1;32m--> 340\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwait_until_idle\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    341\u001B[0m \u001B[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001B[39;00m\n\u001B[0;32m    342\u001B[0m \u001B[38;5;66;03m# output stream names.\u001B[39;00m\n\u001B[0;32m    343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_stream_type_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-20T08:24:56.119979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#################### Pose Identify Using Values\n",
    "def calculateAngle(landmark1, landmark2, landmark3):\n",
    "    '''\n",
    "    This function calculates angle between three different landmarks.\n",
    "    Args:\n",
    "        landmark1: The first landmark containing the x,y and z coordinates.\n",
    "        landmark2: The second landmark containing the x,y and z coordinates.\n",
    "        landmark3: The third landmark containing the x,y and z coordinates.\n",
    "    Returns:\n",
    "        angle: The calculated angle between the three landmarks.\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Get the required landmarks coordinates.\n",
    "    x1, y1, _ = landmark1\n",
    "    x2, y2, _ = landmark2\n",
    "    x3, y3, _ = landmark3\n",
    "\n",
    "    # Calculate the angle between the three points\n",
    "    angle = math.degrees(math.atan2(y3 - y2, x3 - x2) - math.atan2(y1 - y2, x1 - x2))\n",
    "    \n",
    "    # Check if the angle is less than zero.\n",
    "    if angle < 0:\n",
    "\n",
    "        # Add 360 to the found angle.\n",
    "        angle += 360\n",
    "    \n",
    "    # Return the calculated angle.\n",
    "    return angle"
   ],
   "id": "39b297ba1e4a8a7d",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-20T08:24:59.922392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Calculate the angle between the three landmarks.\n",
    "angle = calculateAngle((558, 326, 0), (642, 333, 0), (718, 321, 0))\n",
    "\n",
    "# Display the calculated angle.\n",
    "print(f'The calculated angle is {angle}')"
   ],
   "id": "945ff21247641fd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated angle is 166.26373169437744\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-01-20T08:25:13.379662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def classifyPose(landmarks, output_image, display=False):\n",
    "    '''\n",
    "    This function classifies yoga poses depending upon the angles of various body joints.\n",
    "    Args:\n",
    "        landmarks: A list of detected landmarks of the person whose pose needs to be classified.\n",
    "        output_image: A image of the person with the detected pose landmarks drawn.\n",
    "        display: A boolean value that is if set to true the function displays the resultant image with the pose label \n",
    "        written on it and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The image with the detected pose landmarks drawn and pose label written.\n",
    "        label: The classified pose label of the person in the output_image.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Initialize the label of the pose. It is not known at this stage.\n",
    "    label = 'Unknown Pose'\n",
    "\n",
    "    # Specify the color (Red) with which the label will be written on the image.\n",
    "    color = (0, 0, 255)\n",
    "    \n",
    "    # Calculate the required angles.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Get the angle between the left shoulder, elbow and wrist points. \n",
    "    left_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value])\n",
    "    \n",
    "    # Get the angle between the right shoulder, elbow and wrist points. \n",
    "    right_elbow_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                                       landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value],\n",
    "                                       landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value])   \n",
    "    \n",
    "    # Get the angle between the left elbow, shoulder and hip points. \n",
    "    left_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value],\n",
    "                                         landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value],\n",
    "                                         landmarks[mp_pose.PoseLandmark.LEFT_HIP.value])\n",
    "\n",
    "    # Get the angle between the right hip, shoulder and elbow points. \n",
    "    right_shoulder_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                                          landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value],\n",
    "                                          landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value])\n",
    "\n",
    "    # Get the angle between the left hip, knee and ankle points. \n",
    "    left_knee_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.LEFT_HIP.value],\n",
    "                                     landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value],\n",
    "                                     landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value])\n",
    "\n",
    "    # Get the angle between the right hip, knee and ankle points \n",
    "    right_knee_angle = calculateAngle(landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value],\n",
    "                                      landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value])\n",
    "    \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the warrior II pose or the T pose.\n",
    "    # As for both of them, both arms should be straight and shoulders should be at the specific angle.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if the both arms are straight.\n",
    "    if left_elbow_angle > 165 and left_elbow_angle < 195 and right_elbow_angle > 165 and right_elbow_angle < 195:\n",
    "\n",
    "        # Check if shoulders are at the required angle.\n",
    "        if left_shoulder_angle > 80 and left_shoulder_angle < 110 and right_shoulder_angle > 80 and right_shoulder_angle < 110:\n",
    "\n",
    "    # Check if it is the warrior II pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "            # Check if one leg is straight.\n",
    "            if left_knee_angle > 165 and left_knee_angle < 195 or right_knee_angle > 165 and right_knee_angle < 195:\n",
    "\n",
    "                # Check if the other leg is bended at the required angle.\n",
    "                if left_knee_angle > 90 and left_knee_angle < 120 or right_knee_angle > 90 and right_knee_angle < 120:\n",
    "\n",
    "                    # Specify the label of the pose that is Warrior II pose.\n",
    "                    label = 'Warrior II Pose' \n",
    "                        \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the T pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "            # Check if both legs are straight\n",
    "            if left_knee_angle > 160 and left_knee_angle < 195 and right_knee_angle > 160 and right_knee_angle < 195:\n",
    "\n",
    "                # Specify the label of the pose that is tree pose.\n",
    "                label = 'T Pose'\n",
    "\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if it is the tree pose.\n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if one leg is straight\n",
    "    if left_knee_angle > 165 and left_knee_angle < 195 or right_knee_angle > 165 and right_knee_angle < 195:\n",
    "\n",
    "        # Check if the other leg is bended at the required angle.\n",
    "        if left_knee_angle > 315 and left_knee_angle < 335 or right_knee_angle > 25 and right_knee_angle < 45:\n",
    "\n",
    "            # Specify the label of the pose that is tree pose.\n",
    "            label = 'Tree Pose'\n",
    "                \n",
    "    #----------------------------------------------------------------------------------------------------------------\n",
    "    \n",
    "    # Check if the pose is classified successfully\n",
    "    if label != 'Unknown Pose':\n",
    "        \n",
    "        # Update the color (to green) with which the label will be written on the image.\n",
    "        color = (0, 255, 0)  \n",
    "    \n",
    "    # Write the label on the output image. \n",
    "    cv2.putText(output_image, label, (10, 30),cv2.FONT_HERSHEY_PLAIN, 2, color, 2)\n",
    "    \n",
    "    # Check if the resultant image is specified to be displayed.\n",
    "    if display:\n",
    "    \n",
    "        # Display the resultant image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the classified label.\n",
    "        return output_image, label"
   ],
   "id": "df9b8ecaf133c789",
   "outputs": [],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
